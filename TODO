
todo:
    - i think i can bootstrap something in a somewhat move-efficient way..? (maybe not compute-efficient...)
    - on a 9x9 board, start with:
        - an empty hash table
        - a randomly initialized 128x10 policy/value network for the tree
        - a randomly initialized 32x4 policy network for the rollout
    - in a loop:
        - start a game:
            - for each move:
                select a leaf = recursive(child = avg(hash_table[zobrist_hash], value) + c*policy*(exploration term))
                use tree policy/value to expand leaf/update parents with value
                use the rollout policy to finish the game ;D
                backprop the rollout result (updating hash table in the process)
            - Note: game advancement in-tree is a random distribution across legal moves, weighted by simulation count
            - Note: game advancement in-rollout is a random distribution across legal moves, weighted by raw rollout policy
        - after the game concludes -- by both players running out of sensible moves:
            - store the original game record
            - generate symmetries and store separately
            - generate encoded state/actions and save to file
            - create a single batch and train both the tree and rollout policies
        - every 1000 games, save checkpoints for the tree and rollout policies and play 1000 games between the current model and a random other model
            - save the deets so we can work out elos and stuff





Just threw everything away because it's been like a year and even an inefficient model would have improved a lot by now.
So I can just write something like the below and run it when I'm not using the computer and it should just get better and better.
At the very least, I should be able to check in from time to time and see unambiguous improvement and, sometimes, play against the latest model :D
Can probably refactor/jigger up the following in the span of a week or so? Could be a nice project in my spare time in September?

todo:
	- i want to put together a really simple, performant network and i don't mind tracking certain things programmatically
	- for the architecture, i'm thinking something like this:
		- inputs:
			- NxNx5 channels (channels=current black/current white/previous black/previous white/player to move)
		- body:
			- residual convolutional stack (maybe 6x96? with squeeze-excitation? do some training experiments)
		- outputs:
			- policy head (i like the embedding to categorical output like they have in agz)
			- value head (predict who will win the game {tanh me=1, opponent=-1?})
	- training loop:
		load current model or create a new one
		while True:
			start a self-play game:
				while game.sensible_moves_available_for_at_least_one_player == True:
					if move_number < area:
						next_move = tree_search.get_move_after_simulations(simulations=area, weighted_sample_across_all_moves=True)
					else:
						next_move = tree_search.get_raw_model_sample(weighted_sample_across_all_moves=True)
			record game:
				replay_buffer.submit_new_game(sfg_of_the_game_we_just_played)
				if replay_buffer.number_of_games(counting_symmetries=False)//1000: (True if we have at least 1000 games stored)
					training_data = replay_buffer.create_training_dataset(batches=1, samples_per_batch=1024, include_symmetries=True)
					model.train(training_data)
					model.save(filename="current_model.h5")
				if replay_buffer.number_of_games(counting_symmetries=False) % 1000 == 0:
					model.create_checkpoint(id=replay_buffer.number_of_games(counting_symmetries=False))
					do a self-play tournament:
						load existing tournament data, which includes information about all model checkpoints
						play 1000 games between randomly selected models, favoring recent models, maybe like int(len(models)*(1-random()**2))
							these games won't use mcts, they'll just do a weighted random sample across all "sensible" moves until nobody has sensible moves left
							so this should all play out way way faster than the normal self-play games and that's okay because we're just evaluating raw models
					produce some kind of report (csv?) of the latest state of the model tournaments and who's the strongest and all that
						i'm thinking about that triangle plot thing that shows which models can beat which other models
	- then basically just let this run whenever you're not actively using the big computer and it should just get stronger over time
		at the very least, it should be possible to see that the models are eventually using strategy and stuff



